{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Check if we're using GPU OR CPU"
      ],
      "metadata": {
        "id": "Yxpq7u7m3pIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
      ],
      "metadata": {
        "id": "4ymrCp1UbVYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing Devices including GPU's with Tensorflow"
      ],
      "metadata": {
        "id": "EpPcIQG1bZne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "euNKqVe-3vlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To Check GPU in Tensorflow\n"
      ],
      "metadata": {
        "id": "WSSuM27ZbhKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "metadata": {
        "id": "JtTyv6x7bQKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Loading Dataset"
      ],
      "metadata": {
        "id": "n1YBrRv42icV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To Load Datasets\n",
        "from tensorflow.keras.datasets import mnist \n",
        "\n",
        "#load the MNIST training and test dataset \n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "NdMOrNJN2oM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Inspecting our Dataset "
      ],
      "metadata": {
        "id": "4X2SpJZb4Jfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the number of samples in x_train, x_test, y_train, y_test \n",
        "print('The shape or dimensions of x_train', str(x_train.shape))\n",
        "\n",
        "#Print the number of samples in our data \n",
        "print('Number of samples in training data: ', str(len(x_train)))\n",
        "print('Number of labels in training data: ', str(len(y_train)))\n",
        "print('Number of samples in test data: ', str(len(x_test)))\n",
        "print('Number of labels in test data: ', str(len(y_test)))"
      ],
      "metadata": {
        "id": "H8Z1EmXF4Dn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Visualizing our image dataset"
      ],
      "metadata": {
        "id": "EMXC3Q_q5wdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's view the 50 first images of the MNIST training dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure and change size\n",
        "figure = plt.figure()\n",
        "plt.figure(figsize=(16,10))\n",
        "\n",
        "# Set how many images we wish to see\n",
        "num_of_images = 50 \n",
        "\n",
        "# iterate index from 1 to 51\n",
        "for index in range(1, num_of_images + 1):\n",
        "    plt.subplot(5, 10, index).set_title(f'{y_train[index]}')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_train[index], cmap='gray_r')"
      ],
      "metadata": {
        "id": "SWk6n_hU5SoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Preprocessing "
      ],
      "metadata": {
        "id": "YghWaEx7ClhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's store the number of rows and columns \n",
        "img_rows = x_train[0].shape[0]\n",
        "img_columns = x_train[0].shape[1]\n",
        "\n",
        "#Getting our data in the right shape needed for keras \n",
        "#We need to add a 4th dimension to our data thereby changing our \n",
        "#Original image shape of (60000, 28, 28) to (60000, 28, 28, 1)\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_columns, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_columns, 1)\n",
        "\n",
        "#store the shape of a single image \n",
        "input_shape = (img_rows, img_columns, 1)\n",
        "\n",
        "#change our image type to float32 data type \n",
        "x_train = x_train.astype('float32') #uint8 originally \n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#Normalize our data by changing the range from (0:255) to (0:1)\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "print(img_rows, img_columns)"
      ],
      "metadata": {
        "id": "d-IIeT2D52-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Hot Encoding (y-labels)"
      ],
      "metadata": {
        "id": "THUxVdqzHlqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Now we one hot encode outputs \n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "#Let's count thee number of columns in our hot encoded matrix \n",
        "print(\"Number of classes: \", str(y_test.shape[1]))\n",
        "\n",
        "num_classes = y_test.shape[1]\n",
        "num_pixels = x_train.shape[1] * x_train.shape[2]"
      ],
      "metadata": {
        "id": "bjChdJCUDtG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at our raw data\n",
        "y_train[0]"
      ],
      "metadata": {
        "id": "1wio91MhEw5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Building Convolutional Neural Network"
      ],
      "metadata": {
        "id": "ilxp2dsyKQLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', strides = (2,2)))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), strides=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = SGD(0.001), metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "c6hgUplMJE6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Training The Model"
      ],
      "metadata": {
        "id": "zElClmfNOV90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 25\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train, \n",
        "                    batch_size = batch_size,\n",
        "                    epochs = epochs,\n",
        "                    verbose = 1,\n",
        "                    validation_data = (x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose = 0)\n",
        "print('Test Loss: ', score[0])\n",
        "print('Test Accuracy: ', score[1])\n",
        "print(\"Total time: \", time.time() - start, \"seconds\") "
      ],
      "metadata": {
        "id": "8yOgfWwmMwkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Ploting Loss and Accuracy"
      ],
      "metadata": {
        "id": "eljf-tMaPuhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting our loss charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the History object we created to get our saved performance results\n",
        "history_dict = history.history\n",
        "\n",
        "# Extract the loss and validation losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "# Get the number of epochs and create an array up to that number using range()\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "# Plot line charts for both Validation and Training Loss\n",
        "line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\n",
        "line2 = plt.plot(epochs, loss_values, label='Training Loss')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m0OwLG0zP6pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting our accuracy charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n",
        "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Up41Tg8wQBMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns \n",
        "\n",
        "# Predict the values from the testing dataset\n",
        "Y_pred = model.predict(x_test)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert testing observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1)\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = tf.math.confusion_matrix(Y_true, Y_pred_classes) \n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt='g')"
      ],
      "metadata": {
        "id": "0ytEZ5HL7sE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Izn2Zc48Pa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}